difference b/w split and token using nlp
adposition=preposition
===============================================================================================================
https://www.youtube.com/watch?v=R-AG4-qZs1A&list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX
https://www.debug.school/rakeshdevcotocus_468/how-to-extract-information-using-nlp-spacy-library-2606
https://www.debug.school/rakeshdevcotocus_468/how-to-extract-information-using-nlp-nltk-library-boe
https://www.debug.school/rakeshdevcotocus_468/advanced-nlp-technique-to-text-processing-representation-and-understanding-29pb
https://www.debug.school/rakeshdevcotocus_468/automating-tasks-by-processing-text-using-nlp-pipeline-1kj4

prompt== code explain in simple way
prompt==write 10 feature of relavant code like
monitor.cotocus@gmail.com / Cotocus$1234

http://localhost:8888/notebooks/nlp_spacy2.ipynb==========================
==================SUMMARY OF DETAIL NOTES===============
FINDING word,sentence,token.text, token.lemma,word.pos_,word.dep_,word.ent_type_,length of word,word.is_stop,word.is_sent_start,sum & avg of length
doc[0], doc[-1], doc[2:5], doc[:-1], doc[2].pos_, doc[4].is_alpha, doc[6].is_stop, doc[2].lemma 
spacy.explain(label),ent.label_
condition==if word.pos_ == "PROPN"||if token.is_digit||doc[-5:][::-1]||token.is_alpha||token.is_punct||token.like_num||token.text.endswith("s")
unique token.lemma_, unique ent.label_
https://www.debug.school/rakeshdevcotocus_468/automating-tasks-by-processing-text-using-nlp-pipeline-1kj4
lemmatizer,ner,tagger(Part-of-Speech (POS) tags to tokens (e.g., noun, verb),parser,textcat,entity_ruler,Matcher,TextSimilarity,SentimentAnalyzer,Combining Pipes,spancat,morphologizer 
WAP Cleans text data by removing special characters, emojis, or unnecessary whitespaces.
wAP Filters out stop words from the text.
Generates word embeddings for tokens
Removes specific unwanted tokens(blacklist_filter badwords)
1.A Custom pipeline to tokenize sentences and highlight entities advanced sentiment analysis models using entity text and entity labels
PREDEFINED LIST-get sentence by iterator over the sentences in the document doc 
get entity text and entity label from inner loop iterates through each entity in the current sentence.
After processing all the sentences and printing the entities, the function returns the original doc object
2.Detecting Long Sentences that exceed a certain word count for readability using len(sent) > max_length:
PREDEFINED LIST-get sentence by iterator over the sentences in the document doc 
get only those sentence text whose length is greater than customized threshold.
After processing all the long sentences and their length , the function returns the original doc object.
3.Overrides the default lemmatizer for specific words 
PREDEFINED LIST-define dictionary for custom lemmas 
Iterate over the tokens in the given document
Check if the token text is in the custom lemmas dictionary 
then token lemmas value equal to  value in the custom_lemmas dictionary. then Print the custom lemma applied to the token  
the function returns the original doc object.
4.Extracts predefined keywords from the text and counts their occurrences.
PREDEFINED METHOD-defines a set called keywords that contains specific words or phrases you want to search for in the input document. 
define dictionary where keys are keywords from the keywords set and values are initial count of each keyword
iterate Over Tokens in the Document then Check if Token is a Keyword
if matches found then : Increment the Keyword Count for particular key where match found
finally the function returns the original doc object.
LANGCHAIN--Define a dynamic prompt for generating List the keywords separated by commas 
PRompt=Analyze  and extract==>\n\n{text}\n\n"==>"List the keywords separated by commas."
then  splits the response string (generated by the model) into list of substring 
convert list to set and returns it
then apply same logic line Normal way
5.Detects and flags blacklisted words or phrases in the text
PREDEFINED METHOD=defines a set contains blacklist words
Create the flagged List Comprehension by iterating over each token in the SpaCy document doc. 
it checks if the token's text  exists in the blacklist
if match is found  concatenate the words in the flagged list into a single string, 
finally the function returns the original doc object.
6.Identifies and censors profane words in the text if profane words find append *** and use join to combine token
PREDEFINEDLIST-defines a set contains profane or badwords words
iterate in predefined badwords list if token text is found in predefined badwords list then append***otherwise append token text,finally convert list into single string using join 
LANGCHAIN--Define a dynamic prompt for generating List of profane words separated by commas. 
PRompt=Analyze  and identify==>\n\n{text}\n\n"==>"List all profane words separated by commas."
then  splits the response string (generated by the model) into list of substring 
convert list to set and returns it
then apply same logic line Normal way
7.Extracts URLs and email addresses from the text. use regx and find all then use doc extension
PREDEFINED METHOD= Define the URL Pattern and email pattern using re compile
Extract URLs and email from spacy document text using findall
 Assign URLs and email to a Custom Extension Attribute do
8.calculates d prints statistics of POS tags in the text.use doc extension dictionary
counter--counter ecialized dictionary from Python's collections module that counts the occurrences of each unique item in an iterable convert list to dictionary
dict()-s the Counter object into a dictionary und assigns it to a custom extension attribute pos_counts on the doc object
NOTE:MUST SEE USE OF dict(),counter(),list
9.Extracts noun phrases from the text and adds them as an attribute. use list
noun_phrases text is extracted by iterating doc.noun_chunks and stored in list using list comprhension
10.Extracts key sections (e.g., Name, Contact Information, Skills, and Education) from resumes. 
PREDEFINED METHOD= Define the email pattern and phone pattern using re compile
Extract email and phone from spacy document text using search
Extract skills based on a predefined list by iterating and apply if else condition
Extract Name (First Occurrence of a PERSON Entity) by iterating  all the entities in doc.ents then check  if the entity's label is "PERSON, then break stmt
assign all attribute phone,email,skills,name to Custom Extension Attribute do
LANGCHAIN--Define a dynamic prompt for generating List the skills separated by commas. 
PRompt=Analyze the following  and extract a list of ==>\n\n{text}\n\n"==>"List the skills separated by commas."
then  splits the response string (generated by the model) into list of substring 
 returns skills
then apply same logic line Normal way
11. multiple resume Extracts key sections (e.g., Name, Contact Information, Skills, and Education) from resumes. name from ent.label ,email and phone from regx and findall skills from predefined array 
12.Identifies the sentiment of product reviews using predefined (Positive, Negative, or Neutral)
PREDEFINEDLIST=defines a two  set contains positive words and negative
if token.text is found in positive words set then increment score from 0
if token.text is found in negative words set then decrement score from 0
Assign Sentiment Label based on score value if else f score > 0: Sentiment is "Positive."If score < 0: Sentiment is "Negative."If score == 0: Sentiment is "Neutral."
LANGCHAIN=Define a dynamic prompt for generating generate two lists of words or phrases positive sentiment and negative sentiments
PROMPT=Given dynamic keyword==> then generate two lists of words or phrases==> then First, list words with positive sentiment. Second, list words with negative sentiment
 then  splits the response string (generated by the model) into list of substring 
finally remove spaces  ang generate seprate list by commas into a list of words using list comprehension
then apply same logic line Normal way
13.Social Media Hashtag and Mention Extractor
PREDEFINEDLIST=LOGIC= token.text.startswith("#") and token.text.startswith("@")
14.FAQ Finder 
 iteate doc.sents ans select those sentence ends with ? and store that sentence as question in list using listcomprhension
 iteate doc.sents ans select those sentence not ends with ? and store that sentence as answer in list using listcomprhension
 constructs dictionary by iterating multiple list using zip and use list comprehension
 it assigns this list to doc._.faq:
15.Legal Clause Extractor
PREDEFINEDLIST=defines a two  set contains key_clauses 
iterate in each sentence using for loop on doc.sents then extract those text form sentence which is available in in key_clause set ,
so iterate in key_clause is an example of nested for loop
then compares two sequences found between outer loop and inner loop
if similarity greater than 0.8  then append that text in list
16.Plagiarism Detector
PREDEFINEDLIST=define list of sentence
iterate in each sentence using for loop on doc.sents
in nested loop iterate in predefined list on document_db 
LANGCHAIN--Define a dynamic prompt for generating List sentences from external sources that are highly similar. 
PRompt=Analyze the following text  and find sentences ==>\n\n{text}\n\n"==>"List sentences from external sources that are highly similar."
then  splits the response string (generated by the model) into list of substring 
 returns skills
17.Geographical Entity Extractor
Extracts country, state, and city names from the text using a predefined list.
define predefined location set
iterating in doc to get token.text but that token.text must be present in predefined location set
LANGCHAIN--Define a dynamic prompt for generating List sentences from external sources that are highly similar. 
PRompt=Analyze the following text and extract all geographical locations ==>such as countries, states, cities, or landmarks==>\n\n{text}\n\n"==>"List the locations separated by commas."
then  splits the response string (generated by the model) into list of substring 
returns locations
18.Gender Detector
define predefined gender-mapping set
iterating in doc to get token.text but that token.text must be present in predefined name_gender_map  dictionary
and then custom mapping to dictionary keys
LANGCHAIN--Define a dynamic prompt for generating Determine the most likely gender of the given name. 
PRompt=Determine the most likely gender of the given name ==>If the gender is unclear, respond with 'Unknown'."
then  splits the response string (generated by the model) into list of substring 
returns locations
20.E-commerce Product Tagger
define two set for product categories and brands
iterating in doc to get token.text but that token.text must be present in predefined categories  set
iterating in doc to get token.text but that token.text must be present in predefined brands  set
18.Medical Term Extractor
medical terms and conditions from text for healthcare from the text using a predefined list.
LANGCHAIN--Define a dynamic prompt for generating List the medical terms separated by commas. 
PRompt=Analyze the following text and extract all medical terms ==>such as diseases, symptoms, and treatments==>\n\n{text}\n\n"==>"List the medical terms separated by commas."
then  splits the response string (generated by the model) into list of substring 
returns locations
MUST SEE SUMMARY image while iterating different form of doc==> difference between while iterating
======================DETAIL notes============
see Summary
for sentence in doc.sents:
    for word in sentence:
        print(word)

[(token.text, token.lemma_) for sentence in doc.sents for token in sentence]
word_tokenize("Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi")
sent_tokenize("Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi")
https://www.debug.school/rakeshdevcotocus_468/how-to-extract-information-using-nlp-spacy-library-2606=================
Extracting Information from 2-dimensional structure
print below all thing after nested for loop
print(f"{word.text} - {word.pos_}")
print(f"{word.text} ({word.dep_}) -> Head: {word.head.text}")
if word.ent_type_:==>print(f"{word.text} - {word.ent_type_}")
//Print Each Sentence and its Length in Words
word_count = len([word for word in sentence])
    print(f"Sentence: {sentence.text} | Length: {word_count} words")
Retrieve Lemmas for Each Word in Each Sentence==>print(f"{word.text} -> Lemma: {word.lemma_}")
Identify Stop Words in Each Sentence==>print(f"{word.text} - Is Stop Word: {word.is_stop}")
Sentence Start Position of Each Word==>print(f"{word.text} - Sentence Start: {word.is_sent_start}")
Identify Words that are Proper Nouns==>if word.pos_ == "PROPN":===>print(f"Proper Noun: {word.text}")
Print Each Sentence and Calculate Average Word Length==>avg_word_length = sum(len(word) for word in sentence) / len(sentence)
print(f"Sentence: {sentence.text} | Average Word Length: {avg_word_length:.2f}")
Extracting Information using index==========
doc = nlp("Dr. Strange loves pav bhaji of Mumbai as it costs only 2$ per plate.")
doc[0], doc[-1], doc[2:5], doc[:-1], doc[2].pos_, doc[4].is_alpha, doc[6].is_stop, doc[2].lemma_
 [token.text for token in doc[::-1]]
 [token.text for token in doc if token.is_digit]
doc[9]==>(token_with_digit.is_digit 
[token.text for token in doc if token.pos_ == "VERB"]
 [token.text for token in doc[-5:][::-1]]
doc[5].is_stop
[token.lemma_ for token in doc[2:6]]
proper_nouns = [token.text for token in doc if token.pos_ == "PROPN"]
alphabetic_tokens = [token.text for token in doc if token.is_alpha]
stop_words = [token.text for token in doc if token.is_stop]==>len(stop_words)
for ent in doc.ents:===> print(f"Entity: {ent.text}, Label: {ent.label_}")
[token.text for token in doc if token.text.endswith("s")]
[token.text.lower() for token in doc if not token.is_punct]
numeric_tokens = [token.text for token in doc if token.like_num]
 [token.text for token in doc if token.is_title]
unique_lemmas = set([token.lemma_ for token in doc if not token.is_punct])
Extract token sentiments
https://www.debug.school/rakeshdevcotocus_468/how-to-extract-information-using-nlp-nltk-library-boe==============
sentences = sent_tokenize("Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi.")
words = word_tokenize("Dr. Strange loves pav bhaji of mumbai.")
words = word_tokenize("Dr. Strange loves pav bhaji of mumbai.")
filtered_words = [word for word in words if word.lower() not in stop_words]
 [stemmer.stem(word) for word in words]
lemmas = [lemmatizer.lemmatize(word, pos="v") for word in words]
words = word_tokenize("Dr. Strange loves pav bhaji of mumbai.")
pos_tags = pos_tag(words)
pos_tags = pos_tag(word_tokenize("Dr. Strange loves pav bhaji of mumbai."))
named_entities = ne_chunk(pos_tags)
Synonyms and Antonyms using WordNet
list(bigrams(words))||list(trigrams(words))

http://127.0.0.1:8888/notebooks/nlp_spacy_pipelines4.ipynb
#different kind of pipname
nlp = spacy.load("en_core_web_sm")===>nlp.pipe_names or nlp.pipeline 
#Token Information like pos ,their explanation ,lemma(base form)
for token in doc:===>print(token, " | ", spacy.explain(token.pos_), " | ", token.lemma_) or  print(token, " | ", token.pos_, " | ", token.lemma_) or  
 print(token.text, " | ", token.dep_, " | ", token.head.text)
#This part identifies named entities (like company names or amounts) in the sentence for particular entity.
unique_entity_labels = set(ent.label_ for ent in doc.ents)
#Get All Entity Types in spaCy’s Language Model(Possible Named Entities)
for label in nlp.get_pipe("ner").labels:==>print(label, ":", spacy.explain(label))
# A short explanation of the entity type. extract and classify entities (specific pieces of information) in the text.
for ent in doc.ents:==print(ent.text, ent.label_) or print(ent.text, " | ", ent.label_, " | ", spacy.explain(ent.label_))
Trained processing pipeline in French
nlp = spacy.load("fr_core_news_sm")
Adding a component to a blank pipeline==>
nlp.add_pipe("sentencizer")  # Add the sentencizer to the pipeline 
nlp.add_pipe("language_detector", last=True)
nlp.add_pipe("custom_component", last=True)

https://www.debug.school/rakeshdevcotocus_468/automating-tasks-by-processing-text-using-nlp-pipeline-1kj4==================

nlp.add_pipe("lemmatizer", config={"mode": "rule"})
nlp.add_pipe("ner", last=True)
nlp.add_pipe("textcat", last=True)
nlp.add_pipe("tagger", last=True)
nlp.add_pipe("parser", last=True)
nlp.add_pipe("textcat", last=True)==>textcat.add_label("POSITIVE")==>textcat.add_label("NEGATIVE")
ruler = nlp.add_pipe("entity_ruler")==>patterns = [{"label": "ORG", "pattern": "OpenAI"}]
matcher = Matcher(nlp.vocab)==>nlp.add_pipe("matcher", last=True)
similarity_pipe = TextSimilarity(nlp)==>nlp.add_pipe(similarity_pipe, last=True)
sentiment_analyzer = SentimentAnalyzer()==>nlp.add_pipe("sentiment_analyzer", last=True)
nlp.add_pipe("morphologizer", last=True)
cleaned_text = " ".join(token.text for token in doc if token.is_alpha)==>nlp.add_pipe(text_cleaner, last=True)
print([token.text for token in doc])==>nlp.add_pipe(custom_component, last=True)
doc = [token for token in doc if not token.is_stop]==>nlp.add_pipe(stopword_filter, last=True)
embeddings = [token.vector for token in doc]===>nlp.add_pipe(word_embeddings, last=True)
tokens = [token for token in doc if token.text.lower() not in blacklist]==>nlp.add_pipe(blacklist_filter, last=True)==>
nlp.add_pipe("sentencizer", first=True)  # Ensure sentences are split first
nlp.add_pipe(sentence_entity_highlighter, last=True)===>print(f" - Entity: {ent.text} ({ent.label_})")
nlp.add_pipe("sentencizer", first=True)
nlp.add_pipe(long_sentence_detector, last=True)==>if len(sent) > max_length:===>print(f"Long sentence: {sent.text} ({len(sent)} words)")
nlp.add_pipe(custom_lemmatizer, last=True)==>overrides the default lemmatizer for specific words==>{"better": "good", "worse": "bad"}
nlp.add_pipe(keyword_extractor, last=True)==>if token.text in keywords:===>keyword_count[token.text] += 1
nlp.add_pipe(blacklist_detector, last=True)==>flagged = [token.text for token in doc if token.text.lower() in blacklist]
nlp.add_pipe('language_detector', last=True)
nlp.add_pipe(profanity_filter, last=True)===>if token.text.lower() in profane_words:==> tokens.append("***") else: tokens.append(token.text)==>doc._.censored_text = " ".join(tokens)
nlp.add_pipe(url_email_extractor, last=True)==> url_pattern = re.compile(r'https?://\S+|www\.\S+')==>email_pattern = re.compile(r'\S+@\S+\.\S+')
    urls = re.findall(url_pattern, doc.text)|| emails = re.findall(email_pattern, doc.text)

===============================LOGIC BASED=======================================
http://localhost:8888/notebooks/nlp_spacy2.ipynb
[(token.text, token.lemma_) for sentence in doc.sents for token in sentence]

https://www.debug.school/rakeshdevcotocus_468/how-to-extract-information-using-nlp-spacy-library-2606
for sentence in doc.sents:==>for word in sentence==>word_count = len([word for word in sentence])==> print(f"Sentence: {sentence.text} | Length: {word_count} words")
avg_word_length = sum(len(word) for word in sentence) / len(sentence)
doc = nlp("Dr. Strange loves pav bhaji of Mumbai as it costs only 2$ per plate.")
 [token.text for token in doc[::-1]]
 [token.text for token in doc if token.is_digit]
[token.text for token in doc if token.pos_ == "VERB"]
 [token.text for token in doc[-5:][::-1]]
 [token.lemma_ for token in doc[2:6]]
alphabetic_tokens = [token.text for token in doc if token.is_alpha]
[token.text for token in doc if token.text.endswith("s")]
[token.text.lower() for token in doc if not token.is_punct]
numeric_tokens = [token.text for token in doc if token.like_num]
 [token.text for token in doc if token.is_title]
unique_lemmas = set([token.lemma_ for token in doc if not token.is_punct])

http://127.0.0.1:8888/notebooks/nlp_spacy_pipelines4.ipynb
unique_entity_labels = set(ent.label_ for ent in doc.ents)
https://www.debug.school/rakeshdevcotocus_468/automating-tasks-by-processing-text-using-nlp-pipeline-1kj4
cleaned_text = " ".join(token.text for token in doc if token.is_alpha)
